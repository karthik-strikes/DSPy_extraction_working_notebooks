{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d28ccf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import json\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import aiofiles\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "703c4eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model configured successfully\n"
     ]
    }
   ],
   "source": [
    "# Set your API key (uncomment and add your key)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Configure DSPy with OpenAI GPT-4o-mini for cost efficiency\n",
    "lm = dspy.LM('gemini/gemini-2.5-pro', max_tokens=20000, temperature=1.0)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "print(\"Language model configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ee292c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "EXTRACTED_DATA_PATH = \"/nlp/data/karthik9/Sprint1/Dental/Data/acute_pain_mds\"\n",
    "GROUND_TRUTH_PATH = \"/nlp/data/karthik9/Sprint1/Dental/Data/jsons/dichotomous_outcomes.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "815c64d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "üìö Loaded ground truth data:\n",
      "   - Total records: 1691\n",
      "   - Unique files: 81\n",
      "   - Sample filenames: ['3848_Cooper', '3846_Cooper', '569_Seymour', '3155_Gay', '2412_Kiersch']\n",
      "üîç Loaded extracted data:\n",
      "   - Total files: 6\n",
      "   - Sample filenames: ['1102_Qi', '1741_Mehlisch', '1789_Matthews', '2275_Kyselovic', '2518_Kellstein', '3641_Daniels']\n",
      "\n",
      "üìä Files in both datasets: 6\n",
      "Sample common files: ['1741_Mehlisch', '2275_Kyselovic', '3641_Daniels', '1102_Qi', '1789_Matthews']\n"
     ]
    }
   ],
   "source": [
    "def load_ground_truth_data(gt_path: str) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"Load ground truth data and group by filename\"\"\"\n",
    "    with open(gt_path, 'r') as f:\n",
    "        gt_data = json.load(f)\n",
    "    \n",
    "    # Group by filename\n",
    "    grouped_gt = defaultdict(list)\n",
    "    for record in gt_data:\n",
    "        filename = record.get('filename', '')\n",
    "        grouped_gt[filename].append(record)\n",
    "    \n",
    "    print(f\"üìö Loaded ground truth data:\")\n",
    "    print(f\"   - Total records: {len(gt_data)}\")\n",
    "    print(f\"   - Unique files: {len(grouped_gt)}\")\n",
    "    print(f\"   - Sample filenames: {list(grouped_gt.keys())[:5]}\")\n",
    "    \n",
    "    return dict(grouped_gt)\n",
    "\n",
    "def load_extracted_results(extracted_path: str) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"Load all DSPy extracted results from _do.json files\"\"\"\n",
    "    extracted_data = {}\n",
    "    \n",
    "    for root, dirs, files in os.walk(extracted_path):\n",
    "        for file in files:\n",
    "            if file.endswith('_do.json'):\n",
    "                # Extract filename without _do.json\n",
    "                filename = file.replace('_do.json', '')\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                        extracted_data[filename] = data if isinstance(data, list) else [data]\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error loading {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"üîç Loaded extracted data:\")\n",
    "    print(f\"   - Total files: {len(extracted_data)}\")\n",
    "    print(f\"   - Sample filenames: {list(extracted_data.keys())[:6]}\")\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "# %%\n",
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "ground_truth = load_ground_truth_data(GROUND_TRUTH_PATH)\n",
    "extracted_results = load_extracted_results(EXTRACTED_DATA_PATH)\n",
    "\n",
    "# Find common files\n",
    "common_files = set(ground_truth.keys()) & set(extracted_results.keys())\n",
    "print(f\"\\nüìä Files in both datasets: {len(common_files)}\")\n",
    "print(f\"Sample common files: {list(common_files)[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea1653f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CompareExtractionResults(dspy.Signature):\n",
    "    \"\"\"Compare DSPy extracted results with ground truth for a specific study file.\"\"\"\n",
    "    \n",
    "    filename = dspy.InputField(desc=\"Name of the study file being analyzed\")\n",
    "    ground_truth_records = dspy.InputField(desc=\"Ground truth records for this study as JSON string\")\n",
    "    extracted_records = dspy.InputField(desc=\"DSPy extracted records for this study as JSON string\")\n",
    "    \n",
    "    comparison_summary = dspy.OutputField(desc=\"Summary comparing extracted vs ground truth: matches, missing, extra records with counts\")\n",
    "    extra_records_analysis = dspy.OutputField(desc=\"Detailed analysis of extra records that shouldn't exist - patterns, likely causes\")\n",
    "    missing_records_analysis = dspy.OutputField(desc=\"Analysis of ground truth records that were missed during extraction\")\n",
    "    accuracy_metrics = dspy.OutputField(desc=\"Precision, recall, F1-score calculations and interpretation\")\n",
    "\n",
    "class IdentifyExtraRecordPatterns(dspy.Signature):\n",
    "    \"\"\"Identify patterns in extra records to understand systematic over-extraction issues.\"\"\"\n",
    "    \n",
    "    extra_records_batch = dspy.InputField(desc=\"Batch of extra records from multiple files as JSON\")\n",
    "    signature_definitions = dspy.InputField(desc=\"Original DSPy signatures used for extraction\")\n",
    "    \n",
    "    common_patterns = dspy.OutputField(desc=\"Common patterns found in extra records across files\")\n",
    "    signature_issues = dspy.OutputField(desc=\"Specific issues with the DSPy signatures that cause over-extraction\")\n",
    "\n",
    "class AnalyzeExtraRecord(dspy.Signature):\n",
    "    \"\"\"Analyze why a specific extra record was extracted incorrectly.\"\"\"\n",
    "    \n",
    "    extra_record = dspy.InputField(desc=\"The specific extra record that was incorrectly extracted\")\n",
    "    ground_truth_context = dspy.InputField(desc=\"Ground truth records for context of what should exist\")\n",
    "    filename = dspy.InputField(desc=\"Study filename for context\")\n",
    "    \n",
    "    root_cause = dspy.OutputField(desc=\"Why this specific record was extracted when it shouldn't exist\")\n",
    "    signature_weakness = dspy.OutputField(desc=\"Which part of DSPy signature logic led to this error\")\n",
    "    fix_recommendation = dspy.OutputField(desc=\"Specific change to prevent this type of over-extraction\")\n",
    "\n",
    "class GenerateSignatureImprovements(dspy.Signature):\n",
    "    \"\"\"Generate improved DSPy signatures based on analysis of extraction errors.\"\"\"\n",
    "    \n",
    "    original_signatures = dspy.InputField(desc=\"Original DSPy signatures that have issues\")\n",
    "    error_analysis_summary = dspy.InputField(desc=\"Summary of all extraction errors and patterns found\")\n",
    "    common_over_extraction_patterns = dspy.InputField(desc=\"Common patterns in over-extracted records\")\n",
    "    \n",
    "    improved_signatures = dspy.OutputField(desc=\"Improved versions of the DSPy signatures with better constraints\")\n",
    "    additional_validation_rules = dspy.OutputField(desc=\"Validation rules to add to modules to prevent over-extraction\")\n",
    "    few_shot_examples = dspy.OutputField(desc=\"Better few-shot examples to include negative cases\")\n",
    "\n",
    "class BatchAnalyzeExtraRecords(dspy.Signature):\n",
    "    \"\"\"Analyze a batch of extra records for efficiency.\"\"\"\n",
    "    \n",
    "    extra_records_batch = dspy.InputField(desc=\"Batch of extra records to analyze\")\n",
    "    ground_truth_batch = dspy.InputField(desc=\"Corresponding ground truth contexts\")\n",
    "    filenames_batch = dspy.InputField(desc=\"Corresponding filenames\")\n",
    "    \n",
    "    batch_analysis = dspy.OutputField(desc=\"Analysis results for the entire batch with root causes and patterns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3191396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncFileComparisonModule(dspy.Module):\n",
    "    \"\"\"Async module to compare extraction results for a single file.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.compare = dspy.ChainOfThought(CompareExtractionResults)\n",
    "        \n",
    "    async def forward(self, filename: str, ground_truth: List[Dict], extracted: List[Dict]):\n",
    "        \"\"\"Async compare extraction results and return analysis.\"\"\"\n",
    "        \n",
    "        # Run in thread pool to avoid blocking\n",
    "        loop = asyncio.get_event_loop()\n",
    "        \n",
    "        def _compare():\n",
    "            gt_json = json.dumps(ground_truth, indent=2)\n",
    "            extracted_json = json.dumps(extracted, indent=2)\n",
    "            \n",
    "            result = self.compare(\n",
    "                filename=filename,\n",
    "                ground_truth_records=gt_json,\n",
    "                extracted_records=extracted_json\n",
    "            )\n",
    "            return result\n",
    "        \n",
    "        result = await loop.run_in_executor(None, _compare)\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            filename=filename,\n",
    "            gt_count=len(ground_truth),\n",
    "            extracted_count=len(extracted),\n",
    "            comparison_summary=result.comparison_summary,\n",
    "            extra_records_analysis=result.extra_records_analysis,\n",
    "            missing_records_analysis=result.missing_records_analysis,\n",
    "            accuracy_metrics=result.accuracy_metrics\n",
    "        )\n",
    "\n",
    "class AsyncExtraRecordAnalysisModule(dspy.Module):\n",
    "    \"\"\"Async module to analyze extra records and find patterns with batching.\"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size: int = 10):\n",
    "        super().__init__()\n",
    "        self.pattern_identifier = dspy.ChainOfThought(IdentifyExtraRecordPatterns)\n",
    "        self.batch_analyzer = dspy.ChainOfThought(BatchAnalyzeExtraRecords)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    async def forward(self, extra_records_batch: List[Dict], signatures: str, \n",
    "                      detailed_records: List[Tuple[Dict, List[Dict], str]] = None):\n",
    "        \"\"\"Async analyze extra records patterns and individual cases with batching.\"\"\"\n",
    "        \n",
    "        loop = asyncio.get_event_loop()\n",
    "        \n",
    "        # Analyze patterns across all extra records\n",
    "        async def _analyze_patterns():\n",
    "            def _pattern_analysis():\n",
    "                batch_json = json.dumps(extra_records_batch, indent=2)\n",
    "                return self.pattern_identifier(\n",
    "                    extra_records_batch=batch_json,\n",
    "                    signature_definitions=signatures\n",
    "                )\n",
    "            return await loop.run_in_executor(None, _pattern_analysis)\n",
    "        \n",
    "        # Batch analyze individual records for efficiency\n",
    "        async def _analyze_individual_batches():\n",
    "            if not detailed_records:\n",
    "                return []\n",
    "            \n",
    "            analyses = []\n",
    "            \n",
    "            # Process records in batches\n",
    "            for i in range(0, len(detailed_records), self.batch_size):\n",
    "                batch = detailed_records[i:i + self.batch_size]\n",
    "                \n",
    "                def _batch_analysis():\n",
    "                    batch_records = [record for record, _, _ in batch]\n",
    "                    batch_gt = [gt for _, gt, _ in batch]\n",
    "                    batch_filenames = [filename for _, _, filename in batch]\n",
    "                    \n",
    "                    return self.batch_analyzer(\n",
    "                        extra_records_batch=json.dumps(batch_records, indent=2),\n",
    "                        ground_truth_batch=json.dumps(batch_gt, indent=2),\n",
    "                        filenames_batch=json.dumps(batch_filenames, indent=2)\n",
    "                    )\n",
    "                \n",
    "                batch_result = await loop.run_in_executor(None, _batch_analysis)\n",
    "                analyses.append(batch_result.batch_analysis)\n",
    "            \n",
    "            return analyses\n",
    "        \n",
    "        # Run both analyses concurrently\n",
    "        pattern_analysis, individual_analyses = await asyncio.gather(\n",
    "            _analyze_patterns(),\n",
    "            _analyze_individual_batches()\n",
    "        )\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            common_patterns=pattern_analysis.common_patterns,\n",
    "            signature_issues=pattern_analysis.signature_issues,\n",
    "            individual_analyses=individual_analyses\n",
    "        )\n",
    "\n",
    "class AsyncSignatureImprovementModule(dspy.Module):\n",
    "    \"\"\"Async module to generate improved signatures based on error analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.improver = dspy.ChainOfThought(GenerateSignatureImprovements)\n",
    "        \n",
    "    async def forward(self, original_signatures: str, error_summary: str, patterns: str):\n",
    "        \"\"\"Async generate improved signatures and validation rules.\"\"\"\n",
    "        \n",
    "        loop = asyncio.get_event_loop()\n",
    "        \n",
    "        def _improve():\n",
    "            return self.improver(\n",
    "                original_signatures=original_signatures,\n",
    "                error_analysis_summary=error_summary,\n",
    "                common_over_extraction_patterns=patterns\n",
    "            )\n",
    "        \n",
    "        result = await loop.run_in_executor(None, _improve)\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            improved_signatures=result.improved_signatures,\n",
    "            additional_validation_rules=result.additional_validation_rules,\n",
    "            few_shot_examples=result.few_shot_examples\n",
    "        )\n",
    "\n",
    "class AsyncComprehensiveAnalysisModule(dspy.Module):\n",
    "    \"\"\"Main async module orchestrating the complete analysis pipeline with concurrency.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_concurrent_files: int = 6, batch_size: int = 10):\n",
    "        super().__init__()\n",
    "        self.file_comparator = AsyncFileComparisonModule()\n",
    "        self.extra_record_analyzer = AsyncExtraRecordAnalysisModule(batch_size)\n",
    "        self.signature_improver = AsyncSignatureImprovementModule()\n",
    "        self.max_concurrent = max_concurrent_files\n",
    "        self.semaphore = asyncio.Semaphore(max_concurrent_files)\n",
    "        \n",
    "    def find_extra_records(self, gt_records: List[Dict], extracted_records: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Find records that exist in extracted but not in ground truth.\"\"\"\n",
    "        \n",
    "        # Create signatures for ground truth records\n",
    "        gt_signatures = set()\n",
    "        for record in gt_records:\n",
    "            signature = (\n",
    "                record.get('First_Author', ''),\n",
    "                record.get('Intervention_Description', ''),\n",
    "                str(record.get('Outcome_Type', '')),\n",
    "                record.get('Follow_Up_Time', ''),\n",
    "                record.get('Adverse_Effect_Specify', '')\n",
    "            )\n",
    "            gt_signatures.add(signature)\n",
    "        \n",
    "        # Find extracted records not in ground truth\n",
    "        extra_records = []\n",
    "        for extracted in extracted_records:\n",
    "            print(extracted)\n",
    "            signature = (\n",
    "                extracted.get('First_Author', ''),\n",
    "                extracted.get('Intervention_Description', ''),\n",
    "                str(extracted.get('Outcome_Type', '')),\n",
    "                extracted.get('Follow_Up_Time', ''),\n",
    "                extracted.get('Adverse_Effect_Specify', '')\n",
    "            )\n",
    "            \n",
    "            if signature not in gt_signatures:\n",
    "                extra_records.append(extracted)\n",
    "        \n",
    "        return extra_records\n",
    "    \n",
    "    async def analyze_single_file_with_semaphore(self, filename: str, \n",
    "                                                ground_truth_data: Dict[str, List[Dict]], \n",
    "                                                extracted_data: Dict[str, List[Dict]]):\n",
    "        \"\"\"Analyze a single file with concurrency control.\"\"\"\n",
    "        \n",
    "        async with self.semaphore:\n",
    "            gt_records = ground_truth_data[filename]\n",
    "            extracted_records = extracted_data[filename]\n",
    "            \n",
    "            # Find extra records\n",
    "            extra_records = self.find_extra_records(gt_records, extracted_records)\n",
    "            \n",
    "            # Run file comparison\n",
    "            file_analysis = await self.file_comparator(filename, gt_records, extracted_records)\n",
    "            \n",
    "            return {\n",
    "                'filename': filename,\n",
    "                'analysis': file_analysis,\n",
    "                'extra_records_count': len(extra_records),\n",
    "                'extra_records': extra_records,\n",
    "                'gt_records': gt_records\n",
    "            }\n",
    "        \n",
    "    async def forward(self, ground_truth_data: Dict[str, List[Dict]], \n",
    "                      extracted_data: Dict[str, List[Dict]], \n",
    "                      original_signatures: str,\n",
    "                      max_files: int = 20):\n",
    "        \"\"\"Run comprehensive async analysis on the datasets.\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Find common files\n",
    "        common_files = list(set(ground_truth_data.keys()) & set(extracted_data.keys()))\n",
    "        analysis_files = common_files[:max_files]\n",
    "        \n",
    "        print(f\"üöÄ Starting async analysis of {len(analysis_files)} files with max {self.max_concurrent} concurrent...\")\n",
    "        \n",
    "        # Step 1: Analyze all files concurrently\n",
    "        print(\"üìã Processing files concurrently...\")\n",
    "        \n",
    "        file_tasks = [\n",
    "            self.analyze_single_file_with_semaphore(filename, ground_truth_data, extracted_data)\n",
    "            for filename in analysis_files\n",
    "        ]\n",
    "        \n",
    "        file_analyses = await asyncio.gather(*file_tasks)\n",
    "        \n",
    "        # Collect all extra records and details\n",
    "        all_extra_records = []\n",
    "        extra_record_details = []\n",
    "        \n",
    "        for file_result in file_analyses:\n",
    "            extra_records = file_result['extra_records']\n",
    "            all_extra_records.extend(extra_records)\n",
    "            \n",
    "            # Collect details for individual analysis (limit per file)\n",
    "            for extra_record in extra_records[:2]:\n",
    "                extra_record_details.append((\n",
    "                    extra_record, \n",
    "                    file_result['gt_records'], \n",
    "                    file_result['filename']\n",
    "                ))\n",
    "        \n",
    "        file_processing_time = time.time() - start_time\n",
    "        print(f\"üìä Files processed in {file_processing_time:.2f}s. Found {len(all_extra_records)} total extra records\")\n",
    "        \n",
    "        if not all_extra_records:\n",
    "            print(\"‚úÖ No extra records found! Your extraction is perfect.\")\n",
    "            return dspy.Prediction(\n",
    "                file_analyses=file_analyses,\n",
    "                extra_records_analysis=dspy.Prediction(common_patterns=\"No extra records found\", signature_issues=\"No issues detected\", individual_analyses=[]),\n",
    "                signature_improvements=dspy.Prediction(improved_signatures=\"No improvements needed\", additional_validation_rules=\"No additional rules needed\", few_shot_examples=\"Current examples are sufficient\"),\n",
    "                summary_stats={'total_files': len(analysis_files), 'total_extra_records': 0, 'files_with_extras': 0},\n",
    "                processing_time=file_processing_time\n",
    "            )\n",
    "        \n",
    "        # Step 2 & 3: Run extra records analysis and signature improvements concurrently\n",
    "        print(\"üîç Analyzing patterns and generating improvements concurrently...\")\n",
    "        \n",
    "        # Create error summary\n",
    "        error_summary = f\"\"\"\n",
    "        Files analyzed: {len(analysis_files)}\n",
    "        Total extra records: {len(all_extra_records)}\n",
    "        Files with extra records: {len([f for f in file_analyses if f['extra_records_count'] > 0])}\n",
    "        Over-extraction rate: {(len(all_extra_records) / sum(f['analysis'].extracted_count for f in file_analyses) * 100):.1f}%\n",
    "        Processing time: {file_processing_time:.2f}s\n",
    "        \"\"\"\n",
    "        \n",
    "        # Run extra record analysis and signature improvement concurrently\n",
    "        extra_analysis_task = self.extra_record_analyzer(\n",
    "            all_extra_records, \n",
    "            original_signatures,\n",
    "            extra_record_details\n",
    "        )\n",
    "        \n",
    "        # We'll run signature improvement after getting patterns\n",
    "        extra_analysis = await extra_analysis_task\n",
    "        \n",
    "        # Now run signature improvement with the patterns\n",
    "        improvements = await self.signature_improver(\n",
    "            original_signatures,\n",
    "            error_summary,\n",
    "            extra_analysis.common_patterns\n",
    "        )\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"‚úÖ Complete analysis finished in {total_time:.2f}s\")\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            file_analyses=file_analyses,\n",
    "            extra_records_analysis=extra_analysis,\n",
    "            signature_improvements=improvements,\n",
    "            summary_stats={\n",
    "                'total_files': len(analysis_files),\n",
    "                'total_extra_records': len(all_extra_records),\n",
    "                'files_with_extras': len([f for f in file_analyses if f['extra_records_count'] > 0]),\n",
    "                'processing_time': total_time,\n",
    "                'file_processing_time': file_processing_time\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea206e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def load_ground_truth_data_async(gt_path: str) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"Async load ground truth data and group by filename\"\"\"\n",
    "    \n",
    "    async with aiofiles.open(gt_path, 'r') as f:\n",
    "        content = await f.read()\n",
    "        gt_data = json.loads(content)\n",
    "    \n",
    "    grouped_gt = defaultdict(list)\n",
    "    for record in gt_data:\n",
    "        filename = record.get('filename', '')\n",
    "        grouped_gt[filename].append(record)\n",
    "    \n",
    "    print(f\"üìö Loaded ground truth: {len(gt_data)} records across {len(grouped_gt)} files\")\n",
    "    return dict(grouped_gt)\n",
    "\n",
    "async def load_single_extracted_file(file_path: str, filename: str) -> Tuple[str, List[Dict]]:\n",
    "    \"\"\"Load a single extracted results file asynchronously (handles wrapped metadata).\"\"\"\n",
    "    try:\n",
    "        async with aiofiles.open(file_path, 'r') as f:\n",
    "            content = await f.read()\n",
    "            data = json.loads(content)\n",
    "\n",
    "            # Case 1: Already a list of records\n",
    "            if isinstance(data, list):\n",
    "                return filename, data\n",
    "\n",
    "            # Case 2: Dict with \"extracted_records\" key\n",
    "            if isinstance(data, dict) and \"extracted_records\" in data:\n",
    "                return filename, data[\"extracted_records\"]\n",
    "\n",
    "            # Case 3: Single dict record (wrap it)\n",
    "            if isinstance(data, dict):\n",
    "                return filename, [data]\n",
    "\n",
    "            # Fallback: empty\n",
    "            return filename, []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading {file_path}: {e}\")\n",
    "        return filename, []\n",
    "\n",
    "\n",
    "async def load_extracted_results_async(extracted_path: str, max_concurrent: int = 10) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"Async load all DSPy extracted results from _do.json files\"\"\"\n",
    "    \n",
    "    # Find all files first\n",
    "    file_tasks = []\n",
    "    for root, dirs, files in os.walk(extracted_path):\n",
    "        for file in files:\n",
    "            if file.endswith('_do.json'):\n",
    "                filename = file.replace('_do.json', '')\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_tasks.append((file_path, filename))\n",
    "    \n",
    "    print(f\"üîç Found {len(file_tasks)} files to load...\")\n",
    "    \n",
    "    # Load files concurrently with semaphore\n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    \n",
    "    async def load_with_semaphore(file_path: str, filename: str):\n",
    "        async with semaphore:\n",
    "            return await load_single_extracted_file(file_path, filename)\n",
    "    \n",
    "    # Load all files concurrently\n",
    "    load_tasks = [load_with_semaphore(file_path, filename) for file_path, filename in file_tasks]\n",
    "    results = await asyncio.gather(*load_tasks)\n",
    "    \n",
    "    # Convert to dictionary\n",
    "    extracted_data = {filename: data for filename, data in results if data}\n",
    "    \n",
    "    print(f\"‚úÖ Loaded extracted data: {len(extracted_data)} files\")\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75acb206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ Starting async analysis...\n",
      "üöÄ Starting comprehensive async DSPy analysis...\n",
      "üì• Loading datasets concurrently...\n",
      "üîç Found 6 files to load...\n",
      "üìö Loaded ground truth: 1691 records across 81 files\n",
      "‚úÖ Loaded extracted data: 6 files\n",
      "‚úÖ Data loaded in 0.35s\n",
      "üöÄ Starting async analysis of 6 files with max 5 concurrent...\n",
      "üìã Processing files concurrently...\n",
      "üìä Files processed in 102.08s. Found 382 total extra records\n",
      "üîç Analyzing patterns and generating improvements concurrently...\n",
      "‚úÖ Complete analysis finished in 254.47s\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def run_comprehensive_analysis():\n",
    "    \"\"\"Main async function to run the complete analysis\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting comprehensive async DSPy analysis...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data concurrently\n",
    "    print(\"üì• Loading datasets concurrently...\")\n",
    "    \n",
    "    ground_truth_task = load_ground_truth_data_async(GROUND_TRUTH_PATH)\n",
    "    extracted_results_task = load_extracted_results_async(EXTRACTED_DATA_PATH, max_concurrent=10)\n",
    "    \n",
    "    ground_truth, extracted_results = await asyncio.gather(\n",
    "        ground_truth_task, \n",
    "        extracted_results_task\n",
    "    )\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Data loaded in {load_time:.2f}s\")\n",
    "    \n",
    "    # Your original signatures\n",
    "    ORIGINAL_SIGNATURES = '''\n",
    "class ExtractStudyMetadata(dspy.Signature):\n",
    "    \"\"\"Extract basic study metadata from medical research paper markdown.\n",
    "    \n",
    "    This extracts core identifying information about the dental pain management study.\n",
    "    \"\"\"\n",
    "    \n",
    "    markdown_content: str = dspy.InputField(desc=\"Full markdown content of the medical research paper\")\n",
    "\n",
    "    first_author: str = dspy.OutputField(\n",
    "        desc=\"Last name of the first author (e.g., 'Cooper'). Extract only the surname.\"\n",
    "    )\n",
    "    \n",
    "    population_code: str = dspy.OutputField(\n",
    "        desc=\"Numeric code representing the study population type. Codes: 1=simple tooth extraction, 2=surgical tooth extraction (third molar/wisdom teeth), 3=surgical tooth extraction (other teeth), 4=pulpitis or its complications. Can be multiple codes separated by commas (e.g., '2, 3')\"\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "class ExtractInterventions(dspy.Signature):\n",
    "    \"\"\"Extract intervention details from medical research paper markdown.\n",
    "    \n",
    "    This extracts information about pain management interventions used in dental studies.\n",
    "    Focus on medication types, dosages, and participant counts.\n",
    "    \"\"\"\n",
    "    \n",
    "    markdown_content: str = dspy.InputField(desc=\"Full markdown content of the medical research paper\")\n",
    "    \n",
    "    interventions_json: str = dspy.OutputField(\n",
    "        desc=\"\"\"JSON string containing list of interventions. Each intervention object must have:\n",
    "        - intervention_code (integer): Numeric code 1-11 where:\n",
    "          1=Ibuprofen 200-400mg + Acetaminophen 500-1000mg\n",
    "          2=Oxycodone 5mg or Codeine 60mg  \n",
    "          3=Acetaminophen 650mg + Oxycodone 10mg\n",
    "          4=Ibuprofen 200mg + Hydrocodone 5mg\n",
    "          5=Hydrocodone 5mg + Acetaminophen 300-325mg\n",
    "          6=Ibuprofen 400mg (fast acting or acid)\n",
    "          7=Tramadol 37.5mg + Acetaminophen 325mg\n",
    "          8=Acetaminophen 500-1000mg\n",
    "          9=Acetaminophen 600-650mg + Codeine 60mg\n",
    "          10=Naproxen 400-440mg\n",
    "          11=Placebo/NA (If its not mentioned as a placebo, then it is NA)\n",
    "          #12=OTHER\n",
    "        - intervention_description (string): Full description with medication name and exact dose (e.g., \"Ibuprofen 400mg\", \"Naproxen sodium 440mg\")\n",
    "        - n_analyzed (integer): Number of participants analyzed for this intervention group\n",
    "        \n",
    "        Example: [{\"intervention_code\": 6, \"intervention_description\": \"Ibuprofen 400mg\", \"n_analyzed\": 40}]\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ExtractAllOutcomes(dspy.Signature):\n",
    "    \"\"\"Extract ALL outcomes from medical research paper for systematic review.\n",
    "    \n",
    "    This implements COMPLETE DATA CAPTURE methodology - extract every data point\n",
    "    including rescue analgesia, adverse events, and other outcomes at all time points.\n",
    "    Focus on dichotomous outcomes from ALL data sources: main text, figures, tables,\n",
    "    and supplementary materials. Include zero-event outcomes (0/N patients).\n",
    "    \"\"\"\n",
    "    \n",
    "    markdown_content: str = dspy.InputField(desc=\"Full markdown content of the medical research paper including supplementary materials\")\n",
    "    intervention_description: str = dspy.InputField(desc=\"Specific intervention to extract outcomes for (e.g., 'Ibuprofen 400mg', 'Placebo')\")\n",
    "    \n",
    "    all_outcomes_json: str = dspy.OutputField(\n",
    "        desc=\"\"\"JSON string containing list of ALL outcomes for the specified intervention. Each outcome object must have:\n",
    "        \n",
    "        MANDATORY FIELDS FOR ALL OUTCOMES:\n",
    "        - outcome_type (integer): Outcome type code where:\n",
    "          1=Rescue analgesia at 6 hours\n",
    "          2=Rescue analgesia at 4 hours  \n",
    "          4=Rescue analgesia for pulpitis population\n",
    "          5=Adverse effects (nausea, vomiting, drowsiness, dizziness, headache, etc.)\n",
    "          6=Other outcomes (pain relief, time to onset, etc.)\n",
    "        - follow_up_time (string): Exact time point when outcome was measured (e.g., \"6 hours\", \"24hrs\", \"4 hours\", \"7 days\")\n",
    "        - n_analyzed (integer): Number of participants analyzed for this specific outcome\n",
    "        - n_events_number (integer): Number of patients who experienced this outcome\n",
    "        - n_events_percentage (float): Percentage of patients who experienced this outcome (e.g., 17.5, 0.6, 2.4, 0.0)\n",
    "        \n",
    "        CONDITIONAL FIELDS:\n",
    "        - adverse_effect_specify (string): Specific adverse effect name if outcome_type=5 (e.g., \"Drowsiness (sleepy, tired)\", \"Paraesthesia oral\", \"Vomiting\"). Use \"NA\" if outcome_type‚â†5\n",
    "        - other_outcome_specify (string): Detailed description if outcome_type=6 (e.g., \"Time to meaningful pain relief\", \"Pain intensity difference\"). Use \"NA\" if outcome_type‚â†6\n",
    "        - adverse_effects_all_study (string): List of all adverse effects if not reported per study arm, or \"NA\" if reported per arm\n",
    "        \n",
    "        DOCUMENTATION FIELDS:\n",
    "        - extraction_notes (string): Technical documentation including data source (\"From Table 2\", \"From Figure 5\", \"From Supplementary Table 3\"), extraction method (\"Direct from table\", \"Interpreted from Kaplan-Meier curve\"), and population used (\"Per-protocol population\", \"Safety population\", \"ITT population\")\n",
    "        - comments (string): Study-specific information including single vs multiple dose design, surgical techniques mentioned, methodological features, dropout rates, calculation details\n",
    "        \n",
    "        EXTRACTION REQUIREMENTS:\n",
    "        - Extract EVERY outcome reported, including zero-event outcomes (0/N)\n",
    "        - Create separate entries for each time point assessment\n",
    "        - Include outcomes from ALL data sources (main text, figures, supplements)\n",
    "        - Use appropriate analysis populations (efficacy vs safety)\n",
    "        - Document any calculations or interpretations performed\n",
    "        \n",
    "        Example: [\n",
    "          {\"outcome_type\": 1, \"follow_up_time\": \"6 hours\", \"n_analyzed\": 40, \"n_events_number\": 15, \"n_events_percentage\": 37.5, \"adverse_effect_specify\": \"NA\", \"other_outcome_specify\": \"NA\", \"adverse_effects_all_study\": \"NA\", \"extraction_notes\": \"From Table 3, per-protocol population\", \"comments\": \"single dose study with overnight monitoring\"},\n",
    "          {\"outcome_type\": 5, \"follow_up_time\": \"24 hours\", \"n_analyzed\": 40, \"n_events_number\": 7, \"n_events_percentage\": 17.5, \"adverse_effect_specify\": \"Drowsiness (sleepy, tired)\", \"other_outcome_specify\": \"NA\", \"adverse_effects_all_study\": \"NA\", \"extraction_notes\": \"From safety table, safety population\", \"comments\": \"mild to moderate severity\"}\n",
    "        ]\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "class StructureComprehensiveOutcome(dspy.Signature):\n",
    "    \"\"\"Structure extracted data into the final comprehensive dichotomous outcome format.\n",
    "    \n",
    "    This combines study metadata, intervention details, and any outcome data (rescue analgesia,\n",
    "    adverse events, or other outcomes) into the standardized format used for systematic review\n",
    "    and meta-analysis. Each record represents one outcome for one intervention in one study.\n",
    "    \"\"\"\n",
    "    \n",
    "    study_metadata_json: str = dspy.InputField(desc=\"Study metadata as JSON string with first_author,  population_code\")\n",
    "    intervention_json: str = dspy.InputField(desc=\"Single intervention details as JSON string with intervention_code, intervention_description, n_analyzed\")\n",
    "    outcome_json: str = dspy.InputField(desc=\"Single outcome details as JSON string with all outcome fields including outcome_type, follow_up_time, n_events_number, etc.\")\n",
    "    \n",
    "    structured_record_json: str = dspy.OutputField(\n",
    "        desc=\"\"\"Complete structured record as JSON string with exactly these fields:\n",
    "        - First_Author (string): First author last name (e.g., \"Cooper\")\n",
    "        - Population (integer): Population code (1-4)\n",
    "        - Intervention_Code (integer): Intervention code (1-11)\n",
    "        - Intervention_Description (string): Full intervention description with dose\n",
    "        - Outcome_Type (integer): Outcome type (1=rescue analgesia 6h, 2=rescue analgesia 4h, 4=rescue analgesia pulpitis, 5=adverse effects, 6=other)\n",
    "        - Outcome_Other_Specify (string): Detailed outcome description for type 6, or empty string for other types\n",
    "        - Follow_Up_Time (string): Time point (e.g., \"24hrs\", \"6 hours\")\n",
    "        - N_Analyzed (integer): Number of participants analyzed\n",
    "        - Adverse_Effect_Specify (string): Specific adverse effect name for type 5, or empty string for other types\n",
    "        - Adverse_Effects_All_Study (string): All study adverse effects if not reported per arm, or empty string\n",
    "        - N_Events_Number (integer): Number of patients with this outcome\n",
    "        - N_Events_Percentage (float): Percentage of patients with this outcome\n",
    "        - Comments (string): Study-specific methodology, design notes, and extraction details\n",
    "        \n",
    "        FIELD MAPPING RULES:\n",
    "        - For outcome_type 1,2,4 (rescue analgesia): Adverse_Effect_Specify=\"\" and Outcome_Other_Specify=\"\"\n",
    "        - For outcome_type 5 (adverse effects): Outcome_Other_Specify=\"\" and Adverse_Effect_Specify=specific adverse event name\n",
    "        - For outcome_type 6 (other outcomes): Adverse_Effect_Specify=\"\" and Outcome_Other_Specify=detailed outcome description\n",
    "        - Always include extraction methodology and data source information in Comments\n",
    "        - Ensure mathematical validation: (N_Events_Number/N_Analyzed)*100 = N_Events_Percentage\n",
    "        - Use appropriate analysis populations (efficacy vs safety) based on outcome type\n",
    "        \n",
    "        Example: {\"First_Author\": \"Cooper\",  \"Population\": 2, \"Intervention_Code\": 10, \"Intervention_Description\": \"Naproxen sodium 440mg\", \"Outcome_Type\": 5, \"Outcome_Other_Specify\": \"\", \"Follow_Up_Time\": \"24hrs\", \"N_Analyzed\": 166, \"Adverse_Effect_Specify\": \"Paraesthesia oral\", \"Adverse_Effects_All_Study\": \"\", \"N_Events_Number\": 1, \"N_Events_Percentage\": 0.6, \"Comments\": \"extracted from supplementary table 3, safety population, single dose study\"}\"\"\"\n",
    "    )\n",
    "'''\n",
    "    \n",
    "    # Initialize and run the async comprehensive analysis\n",
    "    analyzer = AsyncComprehensiveAnalysisModule(\n",
    "        max_concurrent_files=5,  # Process 5 files concurrently\n",
    "        batch_size=10           # Batch size for extra record analysis\n",
    "    )\n",
    "    \n",
    "    # Run analysis\n",
    "    results = await analyzer(\n",
    "        ground_truth_data=ground_truth,\n",
    "        extracted_data=extracted_results, \n",
    "        original_signatures=ORIGINAL_SIGNATURES,\n",
    "        max_files=10  # Adjust based on your needs\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# %%\n",
    "# Run the async analysis\n",
    "print(\"üé¨ Starting async analysis...\")\n",
    "\n",
    "# For Jupyter notebooks, you might need to handle the event loop\n",
    "try:\n",
    "    # If running in Jupyter, use this approach\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    results = asyncio.run(run_comprehensive_analysis())\n",
    "except RuntimeError:\n",
    "    # If event loop is already running, use this\n",
    "    results = await run_comprehensive_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0de882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ ASYNC COMPREHENSIVE ANALYSIS RESULTS\n",
      "=======================================================\n",
      "‚ö° Performance Metrics:\n",
      "   Total Processing Time: 254.47s\n",
      "   File Processing Time: 102.08s\n",
      "   Files Analyzed: 6\n",
      "\n",
      "üìä Analysis Summary:\n",
      "   Total Extra Records Found: 382\n",
      "   Files with Extra Records: 6\n",
      "   Over-extraction Rate: 100.0%\n",
      "\n",
      "üìã FILE-BY-FILE BREAKDOWN\n",
      "------------------------------\n",
      "\n",
      "üìÑ 1741_Mehlisch:\n",
      "   Ground Truth: 42\n",
      "   Extracted: 112\n",
      "   Extra Records: 112\n",
      "   Metrics: - **True Positives (TP):** 42\n",
      "- **False Positives (FP):** 64\n",
      "- **False Negatives (FN):** 0\n",
      "\n",
      "- **Prec...\n",
      "\n",
      "üìÑ 2275_Kyselovic:\n",
      "   Ground Truth: 24\n",
      "   Extracted: 30\n",
      "   Extra Records: 30\n",
      "   Metrics: - **True Positives (TP):** 24 (records that are in both ground truth and extracted)\n",
      "- **False Positi...\n",
      "\n",
      "üìÑ 3641_Daniels:\n",
      "   Ground Truth: 24\n",
      "   Extracted: 43\n",
      "   Extra Records: 43\n",
      "   Metrics: - **True Positives (TP)**: 24 (records that are in both ground truth and extracted)\n",
      "- **False Positi...\n",
      "\n",
      "üìÑ 1102_Qi:\n",
      "   Ground Truth: 45\n",
      "   Extracted: 98\n",
      "   Extra Records: 98\n",
      "   Metrics: - **True Positives (TP):** 46\n",
      "- **False Positives (FP):** 56\n",
      "- **False Negatives (FN):** 0\n",
      "\n",
      "- **Prec...\n",
      "\n",
      "üìÑ 1789_Matthews:\n",
      "   Ground Truth: 2\n",
      "   Extracted: 9\n",
      "   Extra Records: 9\n",
      "   Metrics: *   **True Positives (TP):** 2 (The number of GT records that were successfully matched)\n",
      "*   **False...\n",
      "\n",
      "üìÑ 2518_Kellstein:\n",
      "   Ground Truth: 50\n",
      "   Extracted: 90\n",
      "   Extra Records: 90\n",
      "   Metrics: - **True Positives (TP):** 50\n",
      "- **False Positives (FP):** 40\n",
      "- **False Negatives (FN):** 0\n",
      "\n",
      "- **Prec...\n",
      "\n",
      "üîç EXTRA RECORDS PATTERN ANALYSIS\n",
      "----------------------------------------\n",
      "Common Patterns:\n",
      "1.  **Atomization of Categorical/Ordinal Outcomes:** Single endpoints with multiple categories are being split into numerous binary records. For example, a patient global assessment scale ('poor', 'fair', 'good', 'very good', 'excellent') is extracted as five separate outcomes. Similarly, a primary endpoint like \"number of successful 24-hour periods\" is broken down into four separate records (\"0 periods\", \"1 period\", \"2 periods\", \"3 periods\").\n",
      "2.  **Exhaustive Extraction of All Adverse Events:** Every single adverse event mentioned in a table or text is extracted as a unique record, regardless of its frequency or clinical relevance (e.g., \"Hypoesthesia oral\", \"Seroma\", \"Epistaxis\"). This leads to a large number of low-frequency or zero-event outcomes.\n",
      "3.  **Multiple Time-Point Extraction from Single Curves:** For outcomes presented in Kaplan-Meier curves (e.g., time to pain relief, time to rescue medication), records are being generated for numerous arbitrary time points along the curve (e.g., pain relief at 0.5h, 1h, 2h, 4h, 6h), rather than extracting data for a single, pre-specified, clinically relevant time point.\n",
      "4.  **Misinterpretation of Median Time-to-Event Data:** Median times from tables (e.g., \"median time to pain relief was 65 minutes\") are being incorrectly converted into a binary outcome, assuming exactly 50% of patients experienced the event at that specific time point.\n",
      "\n",
      "Signature Issues:\n",
      "The primary source of over-extraction is the `ExtractAllOutcomes` signature.\n",
      "\n",
      "1.  **Overly Broad Instructions:** The core instruction to \"**Extract ALL outcomes**\" using a \"**COMPLETE DATA CAPTURE methodology**\" is too permissive. It lacks the necessary constraints to guide the model on what constitutes a distinct, analyzable endpoint. It encourages the model to treat every number associated with an outcome description as a separate record.\n",
      "2.  **Lack of Aggregation Rules for Categorical Data:** The signature does not instruct the model on how to handle categorical or ordinal data. It should contain rules to identify and create a single \"responder\" analysis by combining relevant categories (e.g., \"combine 'good', 'very good', and 'excellent' into a single outcome for positive global assessment\") instead of atomizing them.\n",
      "3.  **No Guidance on Time-to-Event Data:** The instruction to \"**Create separate entries for each time point assessment**\" is problematic when applied to continuous data representations like Kaplan-Meier curves. The signature should specify that only pre-defined, primary time points (e.g., 4 hours, 6 hours) should be extracted, rather than allowing the model to sample multiple points from a curve.\n",
      "4.  **Undefined Scope for Adverse Events:** The signature prompts for specific adverse events but provides no rules for prioritization. This leads to the extraction of every event listed. The signature should be refined to prioritize summary AE data (e.g., \"Any AE\", \"Treatment-related AE\", \"Severe AE\") and a limited list of common, pre-specified AEs (e.g., Nausea, Vomiting, Dizziness, Headache).\n",
      "\n",
      "üî¨ INDIVIDUAL EXTRA RECORD ANALYSIS SAMPLES\n",
      "---------------------------------------------\n",
      "\n",
      "Batch 1 Analysis:\n",
      "**Overall Analysis:**\n",
      "This batch is highly inefficient, with 70% (7 out of 10) of the extra records being duplicates of data already present in the ground truth. Only 3 records were genuinely new data points. The duplicates were missed due to a lack of robustness in the matching logic, which fails to account for common, minor variations in data reporting.\n",
      "\n",
      "**Root Causes and Patterns:**\n",
      "\n",
      "1.  **Semantic Variation:** The primary reason for missed duplicates is the system's inability to match record...[truncated]\n",
      "\n",
      "Batch 2 Analysis:\n",
      "- **Primary Root Cause:** Data Extracted from Figure\n",
      "- **Analysis of Batch:**\n",
      "The batch consists of two `New Valid Record` instances. Both records were correctly extracted from a Kaplan-Meier curve in \"Figure 5\", representing \"time to treatment failure\" at two different time points (4 hours and 6 hours). The ground truth for this file only contains adverse event data (likely from a table) and completely lacks any information on treatment failure or use of rescue medication. This demonstrates a v...[truncated]\n",
      "\n",
      "üí° SIGNATURE IMPROVEMENT RECOMMENDATIONS\n",
      "---------------------------------------------\n",
      "Improved Signatures:\n",
      "```python\n",
      "class ExtractStudyMetadata(dspy.Signature):\n",
      "    \"\"\"Extract basic study metadata from medical research paper markdown.\n",
      "    \n",
      "    This extracts core identifying information about the dental pain management study.\n",
      "    \"\"\"\n",
      "    \n",
      "    markdown_content: str = dspy.InputField(desc=\"Full markdown content of the medical research paper\")\n",
      "\n",
      "    first_author: str = dspy.OutputField(\n",
      "        desc=\"Last name of the first author (e.g., 'Cooper'). Extract only the surname.\"\n",
      "    )\n",
      "    \n",
      "    population_code: str = dspy.OutputField(\n",
      "        desc=\"Numeric code representing the study population type. Codes: 1=simple tooth extraction, 2=surgical tooth extraction (third molar/wisdom teeth), 3=surgical tooth extraction (other teeth), 4=pulpitis or its complications. Can be multiple codes separated by commas (e.g., '2, 3')\"\n",
      "    )\n",
      "    \n",
      "\n",
      "\n",
      "class ExtractInterventions(dspy.Signature):\n",
      "    \"\"\"Extract intervention details from medical research paper markdown.\n",
      "    \n",
      "    This extracts information about pain management interventions used in dental studies.\n",
      "    Focus on medication types, dosages, and participant counts.\n",
      "    \"\"\"\n",
      "    \n",
      "    markdown_content: str = dspy.InputField(desc=\"Full markdown content of the medical research paper\")\n",
      "    \n",
      "    interventions_json: str = dspy.OutputField(\n",
      "        desc=\"\"\"JSON string containing list of interventions. Each intervention object must have:\n",
      "        - intervention_code (integer): Numeric code 1-11 where:\n",
      "          1=Ibuprofen 200-400mg + Acetaminophen 500-1000mg\n",
      "          2=Oxycodone 5mg or Codeine 60mg  \n",
      "          3=Acetaminophen 650mg + Oxycodone 10mg\n",
      "          4=Ibuprofen 200mg + Hydrocodone 5mg\n",
      "          5=Hydrocodone 5mg + Acetaminophen 300-325mg\n",
      "          6=Ibuprofen 400mg (fast acting or acid)\n",
      "          7=Tramadol 37.5mg + Acetaminophen 325mg\n",
      "          8=Acetaminophen 500-1000mg\n",
      "          9=Acetaminophen 600-650mg + Codeine 60mg\n",
      "          10=Naproxen 400-440mg\n",
      "          11=Placebo/NA (If its not mentioned as a placebo, then it is NA)\n",
      "          #12=OTHER\n",
      "        - intervention_description (string): Full description with medication name and exact dose (e.g., \"Ibuprofen 400mg\", \"Naproxen sodium 440mg\")\n",
      "        - n_analyzed (integer): Number of participants analyzed for this intervention group\n",
      "        \n",
      "        Example: [{\"intervention_code\": 6, \"intervention_description\": \"Ibuprofen 400mg\", \"n_analyzed\": 40}]\"\"\"\n",
      "    )\n",
      "\n",
      "\n",
      "class ExtractAllOutcomes(dspy.Signature):\n",
      "    \"\"\"Extract key dichotomous outcomes from medical research paper for systematic review.\n",
      "    \n",
      "    This implements a FOCUSED DATA CAPTURE methodology. Extract only pre-specified, clinically\n",
      "    relevant dichotomous outcomes (event vs. no event) at specific time points. Adhere strictly\n",
      "    to the extraction rules to avoid over-extraction.\n",
      "    \"\"\"\n",
      "    \n",
      "    markdown_content: str = dspy.InputField(desc=\"Full markdown content of the medical research paper including supplementary materials\")\n",
      "    intervention_description: str = dspy.InputField(desc=\"Specific intervention to extract outcomes for (e.g., 'Ibuprofen 400mg', 'Placebo')\")\n",
      "    \n",
      "    all_outcomes_json: str = dspy.OutputField(\n",
      "        desc=\"\"\"JSON string containing a list of KEY outcomes for the specified intervention. Each outcome object must represent a valid dichotomous outcome.\n",
      "\n",
      "        MANDATORY FIELDS FOR ALL OUTCOMES:\n",
      "        - outcome_type (integer): Outcome type code where:\n",
      "          1=Rescue analgesia (at a specific time point)\n",
      "          2=Pain Relief (dichotomized from a scale, e.g., '50% or more pain relief')\n",
      "          3=Any adverse event (a summary measure if available)\n",
      "          4=Serious adverse event (a summary measure if available)\n",
      "          5=Specific adverse event of interest (e.g., Nausea, Vomiting)\n",
      "          6=Other pre-specified dichotomous outcome\n",
      "        - follow_up_time (string): Exact time point when outcome was measured (e.g., \"6 hours\", \"24hrs\")\n",
      "        - n_analyzed (integer): Number of participants analyzed for this specific outcome\n",
      "        - n_events_number (integer): Number of patients who experienced this outcome\n",
      "        - n_events_percentage (float): Percentage of patients who experienced this outcome (e.g., 17.5, 0.6, 2.4, 0.0)\n",
      "        - adverse_effect_specify (string): Specific adverse effect name ONLY if outcome_type=5 (e.g., \"Nausea\"). Use \"NA\" otherwise.\n",
      "        - other_outcome_specify (string): Detailed description ONLY if outcome_type=6 or 2 (e.g., \"Pain relief >= 50% from baseline\", \"Patient Global Assessment: Good or better\"). Use \"NA\" otherwise.\n",
      "        - extraction_notes (string): Data source (\"From Table 2\"), population (\"Per-protocol population\"), and any transformations (\"Dichotomized from 5-point scale\").\n",
      "\n",
      "        **CRITICAL EXTRACTION RULES TO PREVENT OVER-EXTRACTION:**\n",
      "\n",
      "        1.  **DO NOT ATOMIZE SCALES:** For ordinal scales (e.g., pain relief rated 'poor' to 'excellent'), identify a single, meaningful binary split (e.g., 'good or better') and extract ONE record for that dichotomized outcome. Do NOT create separate records for each category. Document the split in 'extraction_notes'.\n",
      "\n",
      "        2.  **PRIORITIZE SUMMARY ADVERSE EVENTS:** If the text provides a summary like \"total adverse events\" or \"any adverse event\", extract that (outcome_type=3). Do NOT extract every single minor adverse event unless no summary is available. Limit extraction to a maximum of 3-4 individual adverse events per intervention group.\n",
      "\n",
      "        3.  **DO NOT SAMPLE FROM CURVES:** Extract data ONLY for explicit time points stated in tables or text. Do NOT interpolate or estimate multiple data points from a Kaplan-Meier curve or other graphs. A single curve should yield data for at most one or two pre-specified time points.\n",
      "\n",
      "        4.  **IGNORE CONTINUOUS SUMMARY DATA:** Do NOT convert medians, means, or other summary statistics (e.g., \"median time to rescue was 65 minutes\") into a dichotomous event count. This data is not a dichotomous outcome and must be ignored.\n",
      "        \n",
      "        Example: [\n",
      "          {\"outcome_type\": 1, \"follow_up_time\": \"6 hours\", \"n_analyzed\": 40, \"n_events_number\": 15, \"n_events_percentage\": 37.5, \"adverse_effect_specify\": \"NA\", \"other_outcome_specify\": \"NA\", \"extraction_notes\": \"From Table 3, per-protocol population\"},\n",
      "          {\"outcome_type\": 3, \"follow_up_time\": \"48 hours\", \"n_analyzed\": 85, \"n_events_number\": 22, \"n_events_percentage\": 25.9, \"adverse_effect_specify\": \"NA\", \"other_outcome_specify\": \"NA\", \"extraction_notes\": \"From text, 'any adverse event', safety population\"},\n",
      "          {\"outcome_type\": 2, \"follow_up_time\": \"4 hours\", \"n_analyzed\": 60, \"n_events_number\": 35, \"n_events_percentage\": 58.3, \"adverse_effect_specify\": \"NA\", \"other_outcome_specify\": \"Patient Global Assessment: Good or better\", \"extraction_notes\": \"From Table 2. Dichotomized from 5-point scale, combining 'Good', 'Very Good', 'Excellent'.\"}\n",
      "        ]\"\"\"\n",
      "    )\n",
      "\n",
      "\n",
      "class StructureComprehensiveOutcome(dspy.Signature):\n",
      "    \"\"\"Structure extracted data into the final comprehensive dichotomous outcome format.\n",
      "    \n",
      "    This combines study metadata, intervention details, and a single outcome data point (rescue analgesia,\n",
      "    adverse events, or other outcomes) into the standardized format used for systematic review\n",
      "    and meta-analysis. Each record represents one outcome for one intervention in one study.\n",
      "    \"\"\"\n",
      "    \n",
      "    study_metadata_json: str = dspy.InputField(desc=\"Study metadata as JSON string with first_author,  population_code\")\n",
      "    intervention_json: str = dspy.InputField(desc=\"Single intervention details as JSON string with intervention_code, intervention_description, n_analyzed\")\n",
      "    outcome_json: str = dspy.InputField(desc=\"Single outcome details as JSON string with all outcome fields including outcome_type, follow_up_time, n_events_number, etc.\")\n",
      "    \n",
      "    structured_record_json: str = dspy.OutputField(\n",
      "        desc=\"\"\"Complete structured record as JSON string with exactly these fields:\n",
      "        - First_Author (string): First author last name (e.g., \"Cooper\")\n",
      "        - Population (integer): Population code (1-4)\n",
      "        - Intervention_Code (integer): Intervention code (1-11)\n",
      "        - Intervention_Description (string): Full intervention description with dose\n",
      "        - Outcome_Type (integer): Outcome type (1=rescue analgesia, 2=pain relief dichotomized, 3=any AE, 4=serious AE, 5=specific AE, 6=other)\n",
      "        - Outcome_Other_Specify (string): Detailed outcome description for type 2 or 6, or empty string for other types.\n",
      "        - Follow_Up_Time (string): Time point (e.g., \"24hrs\", \"6 hours\")\n",
      "        - N_Analyzed (integer): Number of participants analyzed\n",
      "        - Adverse_Effect_Specify (string): Specific adverse effect name for type 5, or empty string for other types.\n",
      "        - N_Events_Number (integer): Number of patients with this outcome\n",
      "        - N_Events_Percentage (float): Percentage of patients with this outcome\n",
      "        - Comments (string): Study-specific methodology and extraction notes (e.g., data source, population, how scales were dichotomized).\n",
      "        \n",
      "        FIELD MAPPING RULES:\n",
      "        - For outcome_type 1,3,4: Adverse_Effect_Specify=\"\" and Outcome_Other_Specify=\"\"\n",
      "        - For outcome_type 5 (specific adverse effects): Outcome_Other_Specify=\"\" and Adverse_Effect_Specify=specific adverse event name\n",
      "        - For outcome_type 2 or 6 (other outcomes): Adverse_Effect_Specify=\"\" and Outcome_Other_Specify=detailed outcome description\n",
      "        - Always include extraction methodology and data source information in Comments.\n",
      "        \n",
      "        Example: {\"First_Author\": \"Cooper\",  \"Population\": 2, \"Intervention_Code\": 10, \"Intervention_Description\": \"Naproxen sodium 440mg\", \"Outcome_Type\": 5, \"Outcome_Other_Specify\": \"\", \"Follow_Up_Time\": \"24hrs\", \"N_Analyzed\": 166, \"Adverse_Effect_Specify\": \"Nausea\", \"N_Events_Number\": 8, \"N_Events_Percentage\": 4.8, \"Comments\": \"Extracted from Table 4, safety population.\"}\"\"\"\n",
      "    )\n",
      "```\n",
      "\n",
      "Additional Validation Rules:\n",
      "```python\n",
      "import re\n",
      "\n",
      "def validate_extractions(records: list[dict]):\n",
      "    \"\"\"\n",
      "    Applies a set of rules to a list of extracted records to flag potential over-extraction.\n",
      "    \"\"\"\n",
      "    alerts = []\n",
      "    \n",
      "    # Group records by study and intervention for contextual checks\n",
      "    grouped_records = {}\n",
      "    for r in records:\n",
      "        key = (r.get(\"First_Author\"), r.get(\"Intervention_Description\"))\n",
      "        if key not in grouped_records:\n",
      "            grouped_records[key] = []\n",
      "        grouped_records[key].append(r)\n",
      "\n",
      "    for key, group in grouped_records.items():\n",
      "        # Rule 1: Detect Atomization of Ordinal Scales\n",
      "        # Looks for multiple records with the same root outcome description (e.g., \"Patient Global Assessment:\")\n",
      "        outcome_roots = {}\n",
      "        for record in group:\n",
      "            if record.get(\"Outcome_Other_Specify\"):\n",
      "                root = record[\"Outcome_Other_Specify\"].split(':')[0].strip()\n",
      "                if root not in outcome_roots:\n",
      "                    outcome_roots[root] = 0\n",
      "                outcome_roots[root] += 1\n",
      "        for root, count in outcome_roots.items():\n",
      "            if count > 1:\n",
      "                alerts.append(f\"ATOMIZATION ALERT for {key}: Found {count} records for outcome '{root}'. Consolidate into a single dichotomized outcome.\")\n",
      "\n",
      "        # Rule 2: Detect Adverse Event Flooding\n",
      "        # Flags if more than 3 specific adverse events are extracted for one intervention.\n",
      "        ae_records = [r for r in group if r.get(\"Outcome_Type\") == 5]\n",
      "        if len(ae_records) > 3:\n",
      "            alerts.append(f\"AE FLOODING ALERT for {key}: Found {len(ae_records)} specific adverse event records. Prioritize extracting 'Any Adverse Event' (Outcome_Type=3).\")\n",
      "            \n",
      "        # Rule 3: Detect Potential Curve Sampling\n",
      "        # Flags if more than 3 finely-grained time points are extracted for the same outcome type.\n",
      "        time_points_by_outcome = {}\n",
      "        for record in group:\n",
      "            outcome_key = (record.get(\"Outcome_Type\"), record.get(\"Outcome_Other_Specify\"), record.get(\"Adverse_Effect_Specify\"))\n",
      "            if outcome_key not in time_points_by_outcome:\n",
      "                time_points_by_outcome[outcome_key] = set()\n",
      "            time_points_by_outcome[outcome_key].add(record.get(\"Follow_Up_Time\"))\n",
      "        for outcome_key, times in time_points_by_outcome.items():\n",
      "            if len(times) > 3:\n",
      "                 alerts.append(f\"CURVE SAMPLING ALERT for {key} on outcome {outcome_key}: Found {len(times)} time points. Ensure these are explicit, pre-specified endpoints and not sampled from a curve.\")\n",
      "\n",
      "    # Rule 4: Detect Median/Mean Misinterpretation (applies to all records)\n",
      "    for record in records:\n",
      "        comments = record.get(\"Comments\", \"\") + record.get(\"extraction_notes\", \"\") # Check both fields\n",
      "        if re.search(r'\\b(median|mean|average time|IQR)\\b', comments, re.IGNORECASE):\n",
      "            alerts.append(f\"MEDIAN MISINTERPRETATION ALERT for {key} at {record.get('Follow_Up_Time')}: Record comments mention a summary statistic. Verify this is a valid dichotomous outcome and not a misinterpretation of continuous data.\")\n",
      "            \n",
      "    return alerts\n",
      "\n",
      "```\n",
      "\n",
      "Better Few-Shot Examples:\n",
      "Here are few-shot examples for the `ExtractAllOutcomes` signature that demonstrate both correct extraction and, crucially, what to avoid.\n",
      "\n",
      "---\n",
      "**Example 1: Handling Ordinal Scales (Demonstrates Rule 1)**\n",
      "\n",
      "**markdown_content:**\n",
      "From the study by Smith et al. on Ibuprofen 400mg (N=60).\n",
      "Table 3: Patient Global Assessment of Efficacy at 4 hours.\n",
      "- Excellent: 5 patients\n",
      "- Very Good: 10 patients\n",
      "- Good: 20 patients\n",
      "- Fair: 15 patients\n",
      "- Poor: 10 patients\n",
      "\n",
      "**intervention_description:** \"Ibuprofen 400mg\"\n",
      "\n",
      "**all_outcomes_json:**\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"outcome_type\": 2, \n",
      "    \"follow_up_time\": \"4 hours\", \n",
      "    \"n_analyzed\": 60, \n",
      "    \"n_events_number\": 35, \n",
      "    \"n_events_percentage\": 58.3, \n",
      "    \"adverse_effect_specify\": \"NA\", \n",
      "    \"other_outcome_specify\": \"Patient Global Assessment: Good or better\", \n",
      "    \"extraction_notes\": \"From Table 3. Dichotomized from 5-point scale, combining 'Good' (20), 'Very Good' (10), and 'Excellent' (5) categories.\"\n",
      "  }\n",
      "]\n",
      "```\n",
      "**REASONING:** The five categories were correctly consolidated into a single dichotomous outcome (\"Good or better\" vs. \"Fair or worse\"). A separate record was NOT created for each category.\n",
      "\n",
      "---\n",
      "**Example 2: Handling Medians (Demonstrates Rule 4)**\n",
      "\n",
      "**markdown_content:**\n",
      "For the placebo group (N=50), the median time to requiring rescue medication was 4.8 hours. In the main text, it is reported that 45 patients (90%) in the placebo group took rescue medication by 24 hours.\n",
      "\n",
      "**intervention_description:** \"Placebo\"\n",
      "\n",
      "**all_outcomes_json:**\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"outcome_type\": 1,\n",
      "    \"follow_up_time\": \"24 hours\",\n",
      "    \"n_analyzed\": 50,\n",
      "    \"n_events_number\": 45,\n",
      "    \"n_events_percentage\": 90.0,\n",
      "    \"adverse_effect_specify\": \"NA\",\n",
      "    \"other_outcome_specify\": \"NA\",\n",
      "    \"extraction_notes\": \"From main text, per-protocol population.\"\n",
      "  }\n",
      "]\n",
      "```\n",
      "**REASONING:** The sentence \"median time... was 4.8 hours\" was correctly IGNORED because it is a continuous summary statistic, not a dichotomous outcome. The extractor correctly identified the valid dichotomous outcome presented separately (\"45 patients... by 24 hours\").\n",
      "\n",
      "---\n",
      "**Example 3: Handling Adverse Events (Demonstrates Rule 2)**\n",
      "\n",
      "**markdown_content:**\n",
      "In the Naproxen group (N=166), a total of 28 patients (16.9%) reported at least one adverse event. The most common events were nausea (8 patients), headache (6 patients), drowsiness (5 patients), and oral paresthesia (1 patient).\n",
      "\n",
      "**intervention_description:** \"Naproxen 440mg\"\n",
      "\n",
      "**all_outcomes_json:**\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"outcome_type\": 3,\n",
      "    \"follow_up_time\": \"48 hours\",\n",
      "    \"n_analyzed\": 166,\n",
      "    \"n_events_number\": 28,\n",
      "    \"n_events_percentage\": 16.9,\n",
      "    \"adverse_effect_specify\": \"NA\",\n",
      "    \"other_outcome_specify\": \"NA\",\n",
      "    \"extraction_notes\": \"From text, summary measure of 'any adverse event', safety population.\"\n",
      "  },\n",
      "  {\n",
      "    \"outcome_type\": 5,\n",
      "    \"follow_up_time\": \"48 hours\",\n",
      "    \"n_analyzed\": 166,\n",
      "    \"n_events_number\": 8,\n",
      "    \"n_events_percentage\": 4.8,\n",
      "    \"adverse_effect_specify\": \"Nausea\",\n",
      "    \"other_outcome_specify\": \"NA\",\n",
      "    \"extraction_notes\": \"From text, safety population.\"\n",
      "  }\n",
      "]\n",
      "```\n",
      "**REASONING:** The extractor correctly prioritized the summary measure \"at least one adverse event\" (Outcome_Type=3). It then extracted only the most common specific AE (Nausea) instead of creating separate records for every minor event mentioned.\n",
      "üíæ Results exported to: dental_dspy_async_analysis.json\n",
      "\n",
      "‚ö° PERFORMANCE ANALYSIS\n",
      "==============================\n",
      "Files Processed: 6\n",
      "Total Time: 254.47s\n",
      "Average Time per File: 17.01s\n",
      "Concurrent Processing: ‚úÖ Enabled\n",
      "\n",
      "üéØ KEY FINDINGS:\n",
      "   - 382 extra records need investigation\n",
      "   - 6 files have over-extraction issues\n",
      "   - Focus on improving signature constraints\n",
      "   - Consider adding validation rules\n",
      "\n",
      "üìã RECOMMENDED NEXT STEPS:\n",
      "1. Review the signature improvements above\n",
      "2. Implement the suggested validation rules\n",
      "3. Add negative examples to few-shot prompts\n",
      "4. Test improved signatures on a subset of files\n",
      "5. Re-run this async analysis to measure improvement\n",
      "\n",
      "‚úÖ Async DSPy Module Analysis Complete!\n",
      "Use the exported JSON for detailed implementation guidance.\n"
     ]
    }
   ],
   "source": [
    "print(\"üéâ ASYNC COMPREHENSIVE ANALYSIS RESULTS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Performance metrics\n",
    "stats = results.summary_stats\n",
    "print(f\"‚ö° Performance Metrics:\")\n",
    "print(f\"   Total Processing Time: {stats['processing_time']:.2f}s\")\n",
    "print(f\"   File Processing Time: {stats['file_processing_time']:.2f}s\")\n",
    "print(f\"   Files Analyzed: {stats['total_files']}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìä Analysis Summary:\")\n",
    "print(f\"   Total Extra Records Found: {stats['total_extra_records']}\")\n",
    "print(f\"   Files with Extra Records: {stats['files_with_extras']}\")\n",
    "if stats['total_extra_records'] > 0:\n",
    "    total_extracted = sum(f['analysis'].extracted_count for f in results.file_analyses)\n",
    "    print(f\"   Over-extraction Rate: {(stats['total_extra_records']/total_extracted)*100:.1f}%\")\n",
    "\n",
    "# File-by-file breakdown\n",
    "print(f\"\\nüìã FILE-BY-FILE BREAKDOWN\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for file_analysis in results.file_analyses:\n",
    "    filename = file_analysis['filename']\n",
    "    analysis = file_analysis['analysis']\n",
    "    extra_count = file_analysis['extra_records_count']\n",
    "    \n",
    "    print(f\"\\nüìÑ {filename}:\")\n",
    "    print(f\"   Ground Truth: {analysis.gt_count}\")\n",
    "    print(f\"   Extracted: {analysis.extracted_count}\")  \n",
    "    print(f\"   Extra Records: {extra_count}\")\n",
    "    \n",
    "    # Show a condensed accuracy metric\n",
    "    accuracy_summary = analysis.accuracy_metrics\n",
    "    if len(accuracy_summary) > 100:\n",
    "        accuracy_summary = accuracy_summary[:100] + \"...\"\n",
    "    print(f\"   Metrics: {accuracy_summary}\")\n",
    "\n",
    "# Extra records pattern analysis (if any extra records exist)\n",
    "if stats['total_extra_records'] > 0:\n",
    "    print(f\"\\nüîç EXTRA RECORDS PATTERN ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Common Patterns:\\n{results.extra_records_analysis.common_patterns}\")\n",
    "    print(f\"\\nSignature Issues:\\n{results.extra_records_analysis.signature_issues}\")\n",
    "\n",
    "    # Individual extra record analyses (show first few)\n",
    "    if results.extra_records_analysis.individual_analyses:\n",
    "        print(f\"\\nüî¨ INDIVIDUAL EXTRA RECORD ANALYSIS SAMPLES\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        for i, analysis_batch in enumerate(results.extra_records_analysis.individual_analyses[:2]):\n",
    "            print(f\"\\nBatch {i+1} Analysis:\")\n",
    "            # Truncate if too long\n",
    "            analysis_text = str(analysis_batch)\n",
    "            if len(analysis_text) > 500:\n",
    "                analysis_text = analysis_text[:500] + \"...[truncated]\"\n",
    "            print(analysis_text)\n",
    "\n",
    "    # Signature improvements\n",
    "    print(f\"\\nüí° SIGNATURE IMPROVEMENT RECOMMENDATIONS\")\n",
    "    print(\"-\" * 45)\n",
    "    print(f\"Improved Signatures:\\n{results.signature_improvements.improved_signatures}\")\n",
    "    print(f\"\\nAdditional Validation Rules:\\n{results.signature_improvements.additional_validation_rules}\")\n",
    "    print(f\"\\nBetter Few-Shot Examples:\\n{results.signature_improvements.few_shot_examples}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ PERFECT EXTRACTION!\")\n",
    "    print(\"No extra records found - your DSPy signatures are working correctly!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Export Async Results\n",
    "\n",
    "# %%\n",
    "async def export_results_async(results, filename: str = \"dental_dspy_async_analysis.json\"):\n",
    "    \"\"\"Async export detailed results\"\"\"\n",
    "    \n",
    "    export_data = {\n",
    "        'analysis_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'performance_metrics': {\n",
    "            'total_processing_time': results.summary_stats['processing_time'],\n",
    "            'file_processing_time': results.summary_stats['file_processing_time'],\n",
    "            'files_processed_concurrently': True,\n",
    "            'max_concurrent_files': 5\n",
    "        },\n",
    "        'summary_statistics': results.summary_stats,\n",
    "        'signature_improvements': {\n",
    "            'improved_signatures': results.signature_improvements.improved_signatures,\n",
    "            'validation_rules': results.signature_improvements.additional_validation_rules,\n",
    "            'few_shot_examples': results.signature_improvements.few_shot_examples\n",
    "        },\n",
    "        'extra_records_patterns': {\n",
    "            'common_patterns': results.extra_records_analysis.common_patterns,\n",
    "            'signature_issues': results.extra_records_analysis.signature_issues\n",
    "        },\n",
    "        'individual_analyses': results.extra_records_analysis.individual_analyses,\n",
    "        'file_analyses_summary': [\n",
    "            {\n",
    "                'filename': f['filename'],\n",
    "                'gt_count': f['analysis'].gt_count,\n",
    "                'extracted_count': f['analysis'].extracted_count,\n",
    "                'extra_count': f['extra_records_count']\n",
    "            }\n",
    "            for f in results.file_analyses\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Export asynchronously\n",
    "    async with aiofiles.open(filename, 'w') as f:\n",
    "        await f.write(json.dumps(export_data, indent=2, default=str))\n",
    "    \n",
    "    print(f\"üíæ Results exported to: {filename}\")\n",
    "\n",
    "# Export results\n",
    "await export_results_async(results)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Performance Analysis and Next Steps\n",
    "\n",
    "# %%\n",
    "print(f\"\\n‚ö° PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "stats = results.summary_stats\n",
    "print(f\"Files Processed: {stats['total_files']}\")\n",
    "print(f\"Total Time: {stats['processing_time']:.2f}s\")\n",
    "print(f\"Average Time per File: {stats['file_processing_time']/stats['total_files']:.2f}s\")\n",
    "print(f\"Concurrent Processing: ‚úÖ Enabled\")\n",
    "\n",
    "if stats['total_extra_records'] > 0:\n",
    "    print(f\"\\nüéØ KEY FINDINGS:\")\n",
    "    print(f\"   - {stats['total_extra_records']} extra records need investigation\")\n",
    "    print(f\"   - {stats['files_with_extras']} files have over-extraction issues\") \n",
    "    print(f\"   - Focus on improving signature constraints\")\n",
    "    print(f\"   - Consider adding validation rules\")\n",
    "    \n",
    "    print(f\"\\nüìã RECOMMENDED NEXT STEPS:\")\n",
    "    print(\"1. Review the signature improvements above\")\n",
    "    print(\"2. Implement the suggested validation rules\")\n",
    "    print(\"3. Add negative examples to few-shot prompts\")\n",
    "    print(\"4. Test improved signatures on a subset of files\")\n",
    "    print(\"5. Re-run this async analysis to measure improvement\")\n",
    "else:\n",
    "    print(f\"\\nüéâ PERFECT RESULTS!\")\n",
    "    print(\"Your DSPy signatures are working perfectly!\")\n",
    "\n",
    "print(f\"\\n‚úÖ Async DSPy Module Analysis Complete!\")\n",
    "print(\"Use the exported JSON for detailed implementation guidance.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
